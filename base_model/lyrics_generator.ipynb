{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2marPuHXy7DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304933a4-f682-4961-eb49-a318b462767e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tnpeqccyrN6",
        "outputId": "b55c9db0-13a3-4c75-fff9-f52f04175071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# import lyricsgenius as lg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.models import load_model\n",
        "\n",
        "try:\n",
        "    nltk.data.find('averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qFLb0RXvyySy"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Songs.csv', sep=',')\n",
        "dataset = dataset.sample(n=300).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X3KGh_fuzIfI"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "for i in range(len(dataset)):\n",
        "  data = data + \"\\n\\n\" + dataset.Lyrics[i].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7JbjXp8_zIkl"
      },
      "outputs": [],
      "source": [
        "class Generator():\n",
        "    \n",
        "    def __init__(self, data, max_seq_len=25):\n",
        "\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.data = data\n",
        "\n",
        "        self.tokenizer = Tokenizer()\n",
        "\n",
        "        self.words = [char for char in sorted(list(set(re.split(r'\\s|\\n|\\n\\n', self.data)))) if char != '']\n",
        "        self.vocabulary = len(self.words)\n",
        "        self.mapped_words = dict((i, c) for i, c in enumerate(self.words))\n",
        "\n",
        "    def stack_layers(self, vocab_size, inputs, outputs):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 160, input_length=self.max_seq_len-1))\n",
        "        model.add(Bidirectional(LSTM(200, return_sequences=True)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(100))\n",
        "        model.add(Dense(vocab_size/2, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "        model.add(Dense(vocab_size, activation='softmax'))\n",
        "        \n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, model, inputs, outputs, callbacks=None, epochs=20):\n",
        "\n",
        "        model.fit(inputs, outputs, epochs=epochs, batch_size=32, shuffle=True, verbose=1, callbacks=callbacks)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def cleaner(self):\n",
        "\n",
        "        lyrics = self.data.split('\\n')\n",
        "\n",
        "        for item in range(len(lyrics)):\n",
        "            lyrics[item] = lyrics[item].rstrip()\n",
        "\n",
        "        lyrics = [item for item in lyrics if item != '']\n",
        "\n",
        "        return lyrics\n",
        "\n",
        "    def tokenize(self):\n",
        "\n",
        "        lyrics = self.cleaner()\n",
        "\n",
        "        self.tokenizer.fit_on_texts(lyrics)\n",
        "\n",
        "        with open('tokenizer.pickle', 'wb') as handle:\n",
        "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        return self.tokenizer, lyrics\n",
        "\n",
        "    def get_sequences(self, tokenizer, lyrics):\n",
        "\n",
        "        seq = []\n",
        "        for item in lyrics:\n",
        "            sequences = tokenizer.texts_to_sequences([item])[0]\n",
        "\n",
        "            for i in range(1, len(sequences)):\n",
        "                n_gram = sequences[:i+1]\n",
        "                seq.append(n_gram)\n",
        "\n",
        "        # max_seq_len = max([len(sequence) for sequence in seq])\n",
        "        seq = np.array(pad_sequences(seq, maxlen=self.max_seq_len, padding='pre'))\n",
        "        vocab_size = len(tokenizer.word_index)+1 # set vocab_size to vocab_size+1 to avoid out of bounds error\n",
        "    \n",
        "        return sequences, seq, vocab_size\n",
        "\n",
        "    def generate(self, model, tokenizer, lyric_length):\n",
        "\n",
        "        idx = [np.random.randint(self.vocabulary)]\n",
        "        seed = [self.mapped_words[idx[-1]]]\n",
        "\n",
        "        for _ in range(lyric_length):\n",
        "            token_list = tokenizer.texts_to_sequences([seed])[0]\n",
        "            token_list = pad_sequences([token_list], maxlen=self.max_seq_len-1, padding='pre')\n",
        "            predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "            predicted = np.random.choice([x for x in range(len(predicted_probs))], p=predicted_probs)\n",
        "\n",
        "            output = \"\"\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if index == predicted:\n",
        "                    output = word\n",
        "                    break\n",
        "\n",
        "            seed += \" \" + output\n",
        "\n",
        "        return ''.join(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7kUVk2ciz6Dh"
      },
      "outputs": [],
      "source": [
        "generator = Generator(data=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YOX4Txcfz_v0"
      },
      "outputs": [],
      "source": [
        "tokenizer, lyrics = generator.tokenize()\n",
        "sequences, seq, vocab_size = generator.get_sequences(tokenizer, lyrics)\n",
        "input_sequences, output_labels = seq[:,:-1], seq[:,-1]\n",
        "one_hot_labels = to_categorical(output_labels, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TDEIOL9NbfC",
        "outputId": "d1d86430-24ae-4353-e0d1-c66cc4d1ca92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73450, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dObzAJv0DQ2",
        "outputId": "f30dfe7e-df70-4fae-aabc-71f80df79865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 6.4006 - accuracy: 0.0487\n",
            "Epoch 1: loss improved from inf to 6.40064, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 935s 402ms/step - loss: 6.4006 - accuracy: 0.0487\n",
            "Epoch 2/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 5.8110 - accuracy: 0.0723\n",
            "Epoch 2: loss improved from 6.40064 to 5.81101, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 909s 396ms/step - loss: 5.8110 - accuracy: 0.0723\n",
            "Epoch 3/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 5.4671 - accuracy: 0.1015\n",
            "Epoch 3: loss improved from 5.81101 to 5.46713, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 908s 395ms/step - loss: 5.4671 - accuracy: 0.1015\n",
            "Epoch 4/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 5.1920 - accuracy: 0.1216\n",
            "Epoch 4: loss improved from 5.46713 to 5.19202, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 911s 397ms/step - loss: 5.1920 - accuracy: 0.1216\n",
            "Epoch 5/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.9570 - accuracy: 0.1432\n",
            "Epoch 5: loss improved from 5.19202 to 4.95701, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 908s 396ms/step - loss: 4.9570 - accuracy: 0.1432\n",
            "Epoch 6/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.7539 - accuracy: 0.1648\n",
            "Epoch 6: loss improved from 4.95701 to 4.75390, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 916s 399ms/step - loss: 4.7539 - accuracy: 0.1648\n",
            "Epoch 7/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.5721 - accuracy: 0.1825\n",
            "Epoch 7: loss improved from 4.75390 to 4.57212, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 911s 397ms/step - loss: 4.5721 - accuracy: 0.1825\n",
            "Epoch 8/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.4019 - accuracy: 0.1996\n",
            "Epoch 8: loss improved from 4.57212 to 4.40192, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 908s 395ms/step - loss: 4.4019 - accuracy: 0.1996\n",
            "Epoch 9/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.2434 - accuracy: 0.2168\n",
            "Epoch 9: loss improved from 4.40192 to 4.24335, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 911s 397ms/step - loss: 4.2434 - accuracy: 0.2168\n",
            "Epoch 10/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 4.0941 - accuracy: 0.2325\n",
            "Epoch 10: loss improved from 4.24335 to 4.09411, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 902s 393ms/step - loss: 4.0941 - accuracy: 0.2325\n",
            "Epoch 11/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.9566 - accuracy: 0.2475\n",
            "Epoch 11: loss improved from 4.09411 to 3.95664, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 913s 398ms/step - loss: 3.9566 - accuracy: 0.2475\n",
            "Epoch 12/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.8293 - accuracy: 0.2639\n",
            "Epoch 12: loss improved from 3.95664 to 3.82928, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 900s 392ms/step - loss: 3.8293 - accuracy: 0.2639\n",
            "Epoch 13/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.7108 - accuracy: 0.2792\n",
            "Epoch 13: loss improved from 3.82928 to 3.71084, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 908s 395ms/step - loss: 3.7108 - accuracy: 0.2792\n",
            "Epoch 14/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.5984 - accuracy: 0.2942\n",
            "Epoch 14: loss improved from 3.71084 to 3.59837, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 920s 400ms/step - loss: 3.5984 - accuracy: 0.2942\n",
            "Epoch 15/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.4950 - accuracy: 0.3083\n",
            "Epoch 15: loss improved from 3.59837 to 3.49500, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 901s 393ms/step - loss: 3.4950 - accuracy: 0.3083\n",
            "Epoch 16/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.3940 - accuracy: 0.3242\n",
            "Epoch 16: loss improved from 3.49500 to 3.39396, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 896s 390ms/step - loss: 3.3940 - accuracy: 0.3242\n",
            "Epoch 17/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.3032 - accuracy: 0.3381\n",
            "Epoch 17: loss improved from 3.39396 to 3.30324, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 897s 391ms/step - loss: 3.3032 - accuracy: 0.3381\n",
            "Epoch 18/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.2147 - accuracy: 0.3520\n",
            "Epoch 18: loss improved from 3.30324 to 3.21469, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 899s 392ms/step - loss: 3.2147 - accuracy: 0.3520\n",
            "Epoch 19/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.1398 - accuracy: 0.3650\n",
            "Epoch 19: loss improved from 3.21469 to 3.13977, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 900s 392ms/step - loss: 3.1398 - accuracy: 0.3650\n",
            "Epoch 20/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 3.0625 - accuracy: 0.3784\n",
            "Epoch 20: loss improved from 3.13977 to 3.06248, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 886s 386ms/step - loss: 3.0625 - accuracy: 0.3784\n",
            "Epoch 21/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.9871 - accuracy: 0.3899\n",
            "Epoch 21: loss improved from 3.06248 to 2.98712, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 889s 387ms/step - loss: 2.9871 - accuracy: 0.3899\n",
            "Epoch 22/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.9183 - accuracy: 0.4026\n",
            "Epoch 22: loss improved from 2.98712 to 2.91827, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 887s 386ms/step - loss: 2.9183 - accuracy: 0.4026\n",
            "Epoch 23/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.8578 - accuracy: 0.4128\n",
            "Epoch 23: loss improved from 2.91827 to 2.85784, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 894s 389ms/step - loss: 2.8578 - accuracy: 0.4128\n",
            "Epoch 24/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.7926 - accuracy: 0.4264\n",
            "Epoch 24: loss improved from 2.85784 to 2.79256, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 889s 387ms/step - loss: 2.7926 - accuracy: 0.4264\n",
            "Epoch 25/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.7407 - accuracy: 0.4338\n",
            "Epoch 25: loss improved from 2.79256 to 2.74066, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 885s 386ms/step - loss: 2.7407 - accuracy: 0.4338\n",
            "Epoch 26/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.6855 - accuracy: 0.4453\n",
            "Epoch 26: loss improved from 2.74066 to 2.68553, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 889s 387ms/step - loss: 2.6855 - accuracy: 0.4453\n",
            "Epoch 27/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.6321 - accuracy: 0.4542\n",
            "Epoch 27: loss improved from 2.68553 to 2.63212, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 881s 384ms/step - loss: 2.6321 - accuracy: 0.4542\n",
            "Epoch 28/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.5860 - accuracy: 0.4632\n",
            "Epoch 28: loss improved from 2.63212 to 2.58596, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 878s 382ms/step - loss: 2.5860 - accuracy: 0.4632\n",
            "Epoch 29/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.5353 - accuracy: 0.4715\n",
            "Epoch 29: loss improved from 2.58596 to 2.53533, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 845s 368ms/step - loss: 2.5353 - accuracy: 0.4715\n",
            "Epoch 30/30\n",
            "2296/2296 [==============================] - ETA: 0s - loss: 2.4912 - accuracy: 0.4795\n",
            "Epoch 30: loss improved from 2.53533 to 2.49125, saving model to base-model.h5\n",
            "2296/2296 [==============================] - 836s 364ms/step - loss: 2.4912 - accuracy: 0.4795\n"
          ]
        }
      ],
      "source": [
        "filepath = 'base-model.h5'\n",
        "callbacks  = [\n",
        "            EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto', restore_best_weights=True),\n",
        "            ModelCheckpoint(filepath=filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "        ]\n",
        "\n",
        "stacked_layers = generator.stack_layers(vocab_size, input_sequences, output_labels)\n",
        "model = generator.train(stacked_layers, input_sequences, one_hot_labels, callbacks=callbacks, epochs=30)\n",
        "\n",
        "# model = keras.models.load_model('base-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('base-model.h5')"
      ],
      "metadata": {
        "id": "mizELVFvlU3W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('base-model.h5')"
      ],
      "metadata": {
        "id": "47wkmkNQljdq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation = generator.generate(model, tokenizer, lyric_length=100)\n",
        "\n",
        "generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Isyo3ewQXkJ4",
        "outputId": "21b4045f-5ab4-4a56-97b3-2a3c93bbbe3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"struggles and the and the and the and the love just just the love love is end on now love go just this all two go yeah pitch don't were come has cup time we goes one has moves love of lies never so better waitin' baby people of romance and the and the and a man now all love just love and just da just can go o11embedshare and deep gonna feel washed in that's time life love of queen come will a few this alone love of afternoon down now she has wave love baby at just oh time\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}